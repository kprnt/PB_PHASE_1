
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
19/02/22 18:22:54 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-C3MTRKT/192.168.1.208
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.2
STARTUP_MSG:   classpath = C:\work\hadoop-2.7.2\etc\hadoop;C:\work\hadoop-2.7.2\share\hadoop\common\lib\activation-1.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\apacheds-i18n-2.0.0-M15.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\apacheds-kerberos-codec-2.0.0-M15.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\api-asn1-api-1.0.0-M20.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\api-util-1.0.0-M20.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\asm-3.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\avro-1.7.4.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-beanutils-1.7.0.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-beanutils-core-1.8.0.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-cli-1.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-codec-1.4.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-collections-3.2.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-compress-1.4.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-configuration-1.6.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-digester-1.8.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-httpclient-3.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-io-2.4.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-lang-2.6.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-logging-1.1.3.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-math3-3.1.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\commons-net-3.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\curator-client-2.7.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\curator-framework-2.7.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\curator-recipes-2.7.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\gson-2.2.4.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\guava-11.0.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\hadoop-annotations-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\hadoop-auth-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\hamcrest-core-1.3.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\htrace-core-3.1.0-incubating.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\httpclient-4.2.5.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\httpcore-4.2.5.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jackson-core-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jackson-jaxrs-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jackson-mapper-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jackson-xc-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\java-xmlbuilder-0.4.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jaxb-api-2.2.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jaxb-impl-2.2.3-1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jersey-core-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jersey-json-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jersey-server-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jets3t-0.9.0.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jettison-1.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jetty-6.1.26.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jetty-util-6.1.26.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jsch-0.1.42.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jsp-api-2.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\jsr305-3.0.0.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\junit-4.11.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\log4j-1.2.17.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\mockito-all-1.8.5.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\netty-3.6.2.Final.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\paranamer-2.3.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\protobuf-java-2.5.0.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\servlet-api-2.5.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\slf4j-api-1.7.10.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\slf4j-log4j12-1.7.10.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\snappy-java-1.0.4.1.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\stax-api-1.0-2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\xmlenc-0.52.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\xz-1.0.jar;C:\work\hadoop-2.7.2\share\hadoop\common\lib\zookeeper-3.4.6.jar;C:\work\hadoop-2.7.2\share\hadoop\common\hadoop-common-2.7.2-tests.jar;C:\work\hadoop-2.7.2\share\hadoop\common\hadoop-common-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\common\hadoop-nfs-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\asm-3.2.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\commons-cli-1.2.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\commons-codec-1.4.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\commons-daemon-1.0.13.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\commons-io-2.4.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\commons-lang-2.6.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\commons-logging-1.1.3.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\guava-11.0.2.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\htrace-core-3.1.0-incubating.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jackson-core-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jackson-mapper-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jersey-core-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jersey-server-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jetty-6.1.26.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jetty-util-6.1.26.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\jsr305-3.0.0.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\leveldbjni-all-1.8.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\log4j-1.2.17.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\netty-3.6.2.Final.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\netty-all-4.0.23.Final.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\protobuf-java-2.5.0.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\servlet-api-2.5.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\xercesImpl-2.9.1.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\xml-apis-1.3.04.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\lib\xmlenc-0.52.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\hadoop-hdfs-2.7.2-tests.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\hadoop-hdfs-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\hdfs\hadoop-hdfs-nfs-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\activation-1.1.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\aopalliance-1.0.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\asm-3.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-cli-1.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-codec-1.4.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-collections-3.2.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-compress-1.4.1.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-io-2.4.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-lang-2.6.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\commons-logging-1.1.3.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\guava-11.0.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\guice-3.0.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\guice-servlet-3.0.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jackson-core-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jackson-jaxrs-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jackson-mapper-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jackson-xc-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\javax.inject-1.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jaxb-api-2.2.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jaxb-impl-2.2.3-1.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jersey-client-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jersey-core-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jersey-guice-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jersey-json-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jersey-server-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jettison-1.1.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jetty-6.1.26.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jetty-util-6.1.26.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\jsr305-3.0.0.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\leveldbjni-all-1.8.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\log4j-1.2.17.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\netty-3.6.2.Final.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\protobuf-java-2.5.0.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\servlet-api-2.5.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\stax-api-1.0-2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\xz-1.0.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\zookeeper-3.4.6-tests.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\lib\zookeeper-3.4.6.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-api-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-applications-distributedshell-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-client-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-common-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-registry-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-applicationhistoryservice-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-common-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-nodemanager-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-resourcemanager-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-sharedcachemanager-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-tests-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\yarn\hadoop-yarn-server-web-proxy-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\aopalliance-1.0.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\asm-3.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\avro-1.7.4.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\commons-compress-1.4.1.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\commons-io-2.4.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\guice-3.0.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\guice-servlet-3.0.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\hadoop-annotations-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\hamcrest-core-1.3.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\jackson-core-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\jackson-mapper-asl-1.9.13.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\javax.inject-1.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\jersey-core-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\jersey-guice-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\jersey-server-1.9.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\junit-4.11.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\leveldbjni-all-1.8.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\log4j-1.2.17.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\netty-3.6.2.Final.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\paranamer-2.3.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\protobuf-java-2.5.0.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\snappy-java-1.0.4.1.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\lib\xz-1.0.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-app-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-common-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-core-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-hs-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-hs-plugins-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-jobclient-2.7.2-tests.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-jobclient-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-client-shuffle-2.7.2.jar;C:\work\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-examples-2.7.2.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r b165c4fe8a74265c792ce23f546c64604acf0e41; compiled by 'jenkins' on 2016-01-26T00:08Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
19/02/22 18:22:54 INFO namenode.NameNode: createNameNode []
19/02/22 18:22:55 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
19/02/22 18:22:55 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
19/02/22 18:22:55 INFO impl.MetricsSystemImpl: NameNode metrics system started
19/02/22 18:22:55 INFO namenode.NameNode: fs.defaultFS is hdfs://localhost:9001
19/02/22 18:22:55 INFO namenode.NameNode: Clients are to use localhost:9001 to access this namenode/service.
19/02/22 18:22:56 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
19/02/22 18:22:56 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
19/02/22 18:22:56 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
19/02/22 18:22:56 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
19/02/22 18:22:56 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
19/02/22 18:22:56 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
19/02/22 18:22:56 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
19/02/22 18:22:56 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
19/02/22 18:22:56 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
19/02/22 18:22:56 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
19/02/22 18:22:56 INFO http.HttpServer2: Jetty bound to port 50070
19/02/22 18:22:56 INFO mortbay.log: jetty-6.1.26
19/02/22 18:22:59 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
19/02/22 18:22:59 WARN common.Util: Path /C:/work/hadoop-2.7.2/data/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
19/02/22 18:22:59 WARN common.Util: Path /C:/work/hadoop-2.7.2/data/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
19/02/22 18:22:59 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
19/02/22 18:22:59 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
19/02/22 18:22:59 WARN common.Util: Path /C:/work/hadoop-2.7.2/data/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
19/02/22 18:22:59 WARN common.Util: Path /C:/work/hadoop-2.7.2/data/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
19/02/22 18:22:59 INFO namenode.FSNamesystem: No KeyProvider found.
19/02/22 18:22:59 INFO namenode.FSNamesystem: fsLock is fair:true
19/02/22 18:22:59 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
19/02/22 18:22:59 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
19/02/22 18:22:59 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
19/02/22 18:22:59 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Feb 22 18:22:59
19/02/22 18:22:59 INFO util.GSet: Computing capacity for map BlocksMap
19/02/22 18:22:59 INFO util.GSet: VM type       = 64-bit
19/02/22 18:23:00 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
19/02/22 18:23:00 INFO util.GSet: capacity      = 2^21 = 2097152 entries
19/02/22 18:23:00 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
19/02/22 18:23:00 INFO blockmanagement.BlockManager: defaultReplication         = 1
19/02/22 18:23:00 INFO blockmanagement.BlockManager: maxReplication             = 512
19/02/22 18:23:00 INFO blockmanagement.BlockManager: minReplication             = 1
19/02/22 18:23:00 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
19/02/22 18:23:00 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
19/02/22 18:23:00 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
19/02/22 18:23:00 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
19/02/22 18:23:00 INFO namenode.FSNamesystem: fsOwner             = kanth (auth:SIMPLE)
19/02/22 18:23:00 INFO namenode.FSNamesystem: supergroup          = supergroup
19/02/22 18:23:00 INFO namenode.FSNamesystem: isPermissionEnabled = true
19/02/22 18:23:00 INFO namenode.FSNamesystem: HA Enabled: false
19/02/22 18:23:00 INFO namenode.FSNamesystem: Append Enabled: true
19/02/22 18:23:01 INFO util.GSet: Computing capacity for map INodeMap
19/02/22 18:23:01 INFO util.GSet: VM type       = 64-bit
19/02/22 18:23:01 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
19/02/22 18:23:01 INFO util.GSet: capacity      = 2^20 = 1048576 entries
19/02/22 18:23:01 INFO namenode.FSDirectory: ACLs enabled? false
19/02/22 18:23:01 INFO namenode.FSDirectory: XAttrs enabled? true
19/02/22 18:23:01 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
19/02/22 18:23:01 INFO namenode.NameNode: Caching file names occuring more than 10 times
19/02/22 18:23:01 INFO util.GSet: Computing capacity for map cachedBlocks
19/02/22 18:23:01 INFO util.GSet: VM type       = 64-bit
19/02/22 18:23:01 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
19/02/22 18:23:01 INFO util.GSet: capacity      = 2^18 = 262144 entries
19/02/22 18:23:01 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
19/02/22 18:23:01 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
19/02/22 18:23:01 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
19/02/22 18:23:01 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
19/02/22 18:23:01 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
19/02/22 18:23:01 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
19/02/22 18:23:01 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
19/02/22 18:23:01 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
19/02/22 18:23:01 INFO util.GSet: Computing capacity for map NameNodeRetryCache
19/02/22 18:23:01 INFO util.GSet: VM type       = 64-bit
19/02/22 18:23:01 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
19/02/22 18:23:01 INFO util.GSet: capacity      = 2^15 = 32768 entries
19/02/22 18:23:01 INFO common.Storage: Lock on C:\work\hadoop-2.7.2\data\namenode\in_use.lock acquired by nodename 14716@DESKTOP-C3MTRKT
19/02/22 18:23:01 INFO namenode.FileJournalManager: Recovering unfinalized segments in C:\work\hadoop-2.7.2\data\namenode\current
19/02/22 18:23:01 INFO namenode.FileJournalManager: Finalizing edits file C:\work\hadoop-2.7.2\data\namenode\current\edits_inprogress_0000000000000000277 -> C:\work\hadoop-2.7.2\data\namenode\current\edits_0000000000000000277-0000000000000000277
19/02/22 18:23:01 INFO namenode.FSImageFormatPBINode: Loading 33 INodes.
19/02/22 18:23:01 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
19/02/22 18:23:01 INFO namenode.FSImage: Loaded image for txid 276 from C:\work\hadoop-2.7.2\data\namenode\current\fsimage_0000000000000000276
19/02/22 18:23:01 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@40844aab expecting start txid #277
19/02/22 18:23:01 INFO namenode.FSImage: Start loading edits file C:\work\hadoop-2.7.2\data\namenode\current\edits_0000000000000000277-0000000000000000277
19/02/22 18:23:01 INFO namenode.EditLogInputStream: Fast-forwarding stream 'C:\work\hadoop-2.7.2\data\namenode\current\edits_0000000000000000277-0000000000000000277' to transaction ID 277
19/02/22 18:23:01 INFO namenode.FSImage: Edits file C:\work\hadoop-2.7.2\data\namenode\current\edits_0000000000000000277-0000000000000000277 of size 1048576 edits # 1 loaded in 0 seconds
19/02/22 18:23:01 INFO namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
19/02/22 18:23:01 INFO namenode.FSImage: Save namespace ...
19/02/22 18:23:02 INFO namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 276
19/02/22 18:23:02 INFO namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=C:\work\hadoop-2.7.2\data\namenode\current\fsimage_0000000000000000100, cpktTxId=0000000000000000100)
19/02/22 18:23:02 INFO namenode.FSEditLog: Starting log segment at 278
19/02/22 18:23:02 INFO namenode.NameCache: initialized with 0 entries 0 lookups
19/02/22 18:23:02 INFO namenode.FSNamesystem: Finished loading FSImage in 1716 msecs
19/02/22 18:23:03 INFO namenode.NameNode: RPC server is binding to localhost:9001
19/02/22 18:23:03 INFO ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
19/02/22 18:23:04 INFO ipc.Server: Starting Socket Reader #1 for port 9001
19/02/22 18:23:04 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean
19/02/22 18:23:04 WARN common.Util: Path /C:/work/hadoop-2.7.2/data/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
19/02/22 18:23:04 INFO namenode.LeaseManager: Number of blocks under construction: 0
19/02/22 18:23:04 INFO namenode.LeaseManager: Number of blocks under construction: 0
19/02/22 18:23:04 INFO hdfs.StateChange: STATE* Safe mode ON.
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 15.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
19/02/22 18:23:04 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
19/02/22 18:23:04 INFO namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9001
19/02/22 18:23:04 INFO namenode.FSNamesystem: Starting services required for active state
19/02/22 18:23:04 INFO ipc.Server: IPC Server listener on 9001: starting
19/02/22 18:23:04 INFO ipc.Server: IPC Server Responder: starting
19/02/22 18:23:04 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
19/02/22 18:23:05 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=a5810f0c-5bd0-4142-81d2-de929584fe39, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7146f39d-d261-4091-888f-c556ca678f66;nsid=1152662001;c=0) storage a5810f0c-5bd0-4142-81d2-de929584fe39
19/02/22 18:23:05 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
19/02/22 18:23:05 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
19/02/22 18:23:05 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
19/02/22 18:23:05 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-26d4f426-43c1-4778-806c-d87dc9672c71 for DN 127.0.0.1:50010
19/02/22 18:23:05 INFO hdfs.StateChange: STATE* Safe mode extension entered.
The reported blocks 14 has reached the threshold 0.9990 of total blocks 15. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
19/02/22 18:23:05 INFO namenode.FSNamesystem: initializing replication queues
19/02/22 18:23:05 INFO BlockStateChange: BLOCK* processReport: from storage DS-26d4f426-43c1-4778-806c-d87dc9672c71 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=a5810f0c-5bd0-4142-81d2-de929584fe39, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-7146f39d-d261-4091-888f-c556ca678f66;nsid=1152662001;c=0), blocks: 15, hasStaleStorage: false, processing time: 45 msecs
19/02/22 18:23:05 INFO blockmanagement.BlockManager: Total number of blocks            = 15
19/02/22 18:23:05 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
19/02/22 18:23:05 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
19/02/22 18:23:05 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
19/02/22 18:23:05 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
19/02/22 18:23:05 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 37 msec
19/02/22 18:23:25 INFO hdfs.StateChange: STATE* Safe mode ON, in safe mode extension.
The reported blocks 15 has reached the threshold 0.9990 of total blocks 15. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
19/02/22 18:23:35 INFO hdfs.StateChange: STATE* Leaving safe mode after 35 secs
19/02/22 18:23:35 INFO hdfs.StateChange: STATE* Safe mode is OFF
19/02/22 18:23:35 INFO hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
19/02/22 18:23:35 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
Feb 22, 2019 6:34:07 PM com.sun.jersey.api.core.PackagesResourceConfig init
INFO: Scanning for root resource and provider classes in the packages:
  org.apache.hadoop.hdfs.server.namenode.web.resources
  org.apache.hadoop.hdfs.web.resources
Feb 22, 2019 6:34:07 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
INFO: Root resource classes found:
  class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
Feb 22, 2019 6:34:07 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
INFO: Provider classes found:
  class org.apache.hadoop.hdfs.web.resources.ExceptionHandler
  class org.apache.hadoop.hdfs.web.resources.UserProvider
Feb 22, 2019 6:34:07 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
Feb 22, 2019 6:34:08 PM com.sun.jersey.spi.inject.Errors processErrorMessages
WARNING: The following warnings have been detected with resource and/or provider classes:
  WARNING: A sub-resource method, public javax.ws.rs.core.Response org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam) throws java.io.IOException,java.lang.InterruptedException, with URI template, "/", is treated as a resource method
  WARNING: A sub-resource method, public javax.ws.rs.core.Response org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.postRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam) throws java.io.IOException,java.lang.InterruptedException, with URI template, "/", is treated as a resource method
  WARNING: A sub-resource method, public javax.ws.rs.core.Response org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.deleteRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.DeleteOpParam,org.apache.hadoop.hdfs.web.resources.RecursiveParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam) throws java.io.IOException,java.lang.InterruptedException, with URI template, "/", is treated as a resource method
  WARNING: A sub-resource method, public javax.ws.rs.core.Response org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.putRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam) throws java.io.IOException,java.lang.InterruptedException, with URI template, "/", is treated as a resource method
19/02/22 18:37:49 INFO namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8
19/02/22 18:37:49 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 127.0.0.1:50010
19/02/22 18:37:49 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741836_1012]
19/02/22 18:37:56 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 127.0.0.1:50010
19/02/22 18:37:58 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741847_1023]
19/02/22 18:38:01 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:50010
19/02/22 18:38:01 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741825_1001]
19/02/22 18:38:33 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741843_1019 127.0.0.1:50010
19/02/22 18:38:34 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741843_1019]
19/02/22 18:38:41 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 127.0.0.1:50010
19/02/22 18:38:43 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741854_1030]
19/02/22 18:38:47 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 127.0.0.1:50010
19/02/22 18:38:49 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741832_1008]
19/02/22 18:41:45 INFO namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 21
19/02/22 18:42:40 INFO hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /input/hashtags.txt._COPYING_
19/02/22 18:42:40 INFO namenode.FSNamesystem: BLOCK* blk_1073741858_1034{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /input/hashtags.txt._COPYING_
19/02/22 18:42:40 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 3155630
19/02/22 18:42:41 INFO hdfs.StateChange: DIR* completeFile: /input/hashtags.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1644207416_1
19/02/22 18:43:38 INFO namenode.FSEditLog: Number of transactions: 15 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 1 Number of syncs: 13 SyncTimes(ms): 29
19/02/22 18:43:38 INFO hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.jar
19/02/22 18:43:38 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:43:38 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.jar is closed by DFSClient_NONMAPREDUCE_-343219059_1
19/02/22 18:43:38 INFO blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.jar
19/02/22 18:43:38 INFO blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.split
19/02/22 18:43:38 INFO hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.split
19/02/22 18:43:38 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:43:38 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.split is closed by DFSClient_NONMAPREDUCE_-343219059_1
19/02/22 18:43:38 INFO hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.splitmetainfo
19/02/22 18:43:38 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:43:38 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-343219059_1
19/02/22 18:43:39 INFO hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.xml
19/02/22 18:43:39 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:43:39 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-343219059_1
19/02/22 18:43:47 INFO hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job_1550881379159_0001_1_conf.xml
19/02/22 18:43:47 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:43:47 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job_1550881379159_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:43:54 INFO hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job_1550881379159_0001_1.jhist
19/02/22 18:43:54 INFO hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job_1550881379159_0001_1.jhist for DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:43:54 INFO ipc.Server: Socket Reader #1 for port 9001: readAndProcess from client 127.0.0.1 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
        at sun.nio.ch.SocketDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
        at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
19/02/22 18:44:00 INFO hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /out/_temporary/1/_temporary/attempt_1550881379159_0001_r_000000_0/part-r-00000
19/02/22 18:44:00 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:44:00 INFO hdfs.StateChange: DIR* completeFile: /out/_temporary/1/_temporary/attempt_1550881379159_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1550881379159_0001_r_000000_0_151838876_1
19/02/22 18:44:00 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:00 INFO hdfs.StateChange: DIR* completeFile: /out/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:00 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:00 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 13138
19/02/22 18:44:00 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0001/job_1550881379159_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:00 INFO hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0001.summary_tmp
19/02/22 18:44:00 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:44:00 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:00 INFO hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0001-1550882619277-kanth-word+count-1550882640840-1-1-SUCCEEDED-default-1550882627651.jhist_tmp
19/02/22 18:44:01 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:44:01 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0001-1550882619277-kanth-word+count-1550882640840-1-1-SUCCEEDED-default-1550882627651.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:01 INFO hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0001_conf.xml_tmp
19/02/22 18:44:01 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:44:01 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-213857493_1
19/02/22 18:44:02 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 127.0.0.1:50010
19/02/22 18:44:02 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741860_1036 127.0.0.1:50010
19/02/22 18:44:02 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741861_1037 127.0.0.1:50010
19/02/22 18:44:02 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741862_1038 127.0.0.1:50010
19/02/22 18:44:02 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 127.0.0.1:50010
19/02/22 18:44:02 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741863_1039 127.0.0.1:50010
19/02/22 18:44:05 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741859_1035, blk_1073741860_1036, blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040]
19/02/22 18:44:07 INFO ipc.Server: Socket Reader #1 for port 9001: readAndProcess from client 127.0.0.1 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
        at sun.nio.ch.SocketDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
        at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
19/02/22 18:54:57 INFO namenode.FSEditLog: Number of transactions: 95 Total time for transactions(ms): 39 Number of transactions batched in Syncs: 1 Number of syncs: 69 SyncTimes(ms): 105
19/02/22 18:54:57 INFO hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /input/urls.txt._COPYING_
19/02/22 18:54:57 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:54:57 INFO hdfs.StateChange: DIR* completeFile: /input/urls.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-62276345_1
19/02/22 18:55:43 INFO ipc.Server: Socket Reader #1 for port 9001: readAndProcess from client 127.0.0.1 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
        at sun.nio.ch.SocketDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
        at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
19/02/22 18:56:06 INFO namenode.FSEditLog: Number of transactions: 101 Total time for transactions(ms): 40 Number of transactions batched in Syncs: 1 Number of syncs: 73 SyncTimes(ms): 111
19/02/22 18:56:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.jar
19/02/22 18:56:06 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:06 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.jar is closed by DFSClient_NONMAPREDUCE_1385757737_1
19/02/22 18:56:06 INFO blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.jar
19/02/22 18:56:06 INFO blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.split
19/02/22 18:56:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.split
19/02/22 18:56:06 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:06 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.split is closed by DFSClient_NONMAPREDUCE_1385757737_1
19/02/22 18:56:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.splitmetainfo
19/02/22 18:56:06 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:06 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1385757737_1
19/02/22 18:56:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.xml
19/02/22 18:56:06 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:06 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job.xml is closed by DFSClient_NONMAPREDUCE_1385757737_1
19/02/22 18:56:13 INFO hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job_1550881379159_0002_1_conf.xml
19/02/22 18:56:13 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:13 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job_1550881379159_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:19 INFO hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job_1550881379159_0002_1.jhist
19/02/22 18:56:19 INFO hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job_1550881379159_0002_1.jhist for DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:24 INFO hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /output/_temporary/1/_temporary/attempt_1550881379159_0002_r_000000_0/part-r-00000
19/02/22 18:56:24 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:24 INFO hdfs.StateChange: DIR* completeFile: /output/_temporary/1/_temporary/attempt_1550881379159_0002_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1550881379159_0002_r_000000_0_-29404191_1
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:25 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 13141
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/kanth/.staging/job_1550881379159_0002/job_1550881379159_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:25 INFO hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0002.summary_tmp
19/02/22 18:56:25 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:25 INFO hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0002-1550883367043-kanth-word+count-1550883385076-1-1-SUCCEEDED-default-1550883373343.jhist_tmp
19/02/22 18:56:25 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0002-1550883367043-kanth-word+count-1550883385076-1-1-SUCCEEDED-default-1550883373343.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:25 INFO hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0002_conf.xml_tmp
19/02/22 18:56:25 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-26d4f426-43c1-4778-806c-d87dc9672c71:NORMAL:127.0.0.1:50010|RBW]]} size 0
19/02/22 18:56:25 INFO hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/kanth/job_1550881379159_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2122325372_1
19/02/22 18:56:26 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741870_1046 127.0.0.1:50010
19/02/22 18:56:26 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741871_1047 127.0.0.1:50010
19/02/22 18:56:26 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741872_1048 127.0.0.1:50010
19/02/22 18:56:26 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741873_1049 127.0.0.1:50010
19/02/22 18:56:26 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741875_1051 127.0.0.1:50010
19/02/22 18:56:26 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741874_1050 127.0.0.1:50010
19/02/22 18:56:27 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741872_1048, blk_1073741873_1049, blk_1073741874_1050, blk_1073741875_1051, blk_1073741870_1046, blk_1073741871_1047]
19/02/22 18:56:31 INFO ipc.Server: Socket Reader #1 for port 9001: readAndProcess from client 127.0.0.1 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
        at sun.nio.ch.SocketDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
        at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)